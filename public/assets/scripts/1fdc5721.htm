<h2 id="ml">ML</h2>
<h3 id="concept">Concept</h3>
<h4 id="ml-1">ML</h4>
<p>From Arthur Samuel</p>
<blockquote>
<p>Machine learning as the field of study that gives computers the ability to learn without being explicitly learned</p>
</blockquote>
<p>From Tom Mitchell</p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
</blockquote>
<h4 id="supervised-learning">Supervised Learning</h4>
<p>You tell the program what is what and it will find the pattern by your guiding.</p>
<p><img src="../../../../public/img/2d99281dfc992452c9d32e022ce71161.png" alt="img"></p>
<h5 id="regression">Regression</h5>
<p>The picture shown above is a <strong>regression problem</strong> to be solved.</p>
<blockquote>
<p><strong>Predict</strong> <strong>continuous</strong> valued output.</p>
</blockquote>
<h5 id="classification">Classification</h5>
<p><img src="../../../../public/img/4f80108ebbb6707d39b7a6da4d2a7a4e.png" alt="img"></p>
<p>The picture shown above is a <strong>classification problem</strong> to be solved.</p>
<blockquote>
<p><strong>Discrete</strong> valued output(there might be more than 2 values).</p>
</blockquote>
<p>*More attributes can be used in the problem.</p>
<p><img src="../../../../public/img/c34fa10153f223aa955d6717663a9f91.png" alt="img"></p>
<h4 id="singlevariate-linear-regression">Singlevariate Linear Regression</h4>
<p>Given an example:</p>
<p><img src="../../../../public/img/44c68412e65e62686a96ad16f278571f.png" alt="img"></p>
<p>To establish notation for future use, we’ll use</p>
<ul>
<li><p>$x^{(i)}$ to denote the “<strong>input</strong>” variables (living area in this example), also called input features.</p>
</li>
<li><p>$ y^{(i)}$ to denote the “<strong>output</strong>” or target variable that we are trying to predict (price). </p>
</li>
<li><p>a pair $(x^{(i)} , y^{(i)} )$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples $(x^{(i)} , y^{(i)}); i = 1, . . . , m$—is called a training set.</p>
</li>
</ul>
<p>We will also use $X$ to denote the space of <strong>input values</strong>, and $Y$ to denote the space of <strong>output values</strong>.</p>
<p>To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function $h : X → Y $ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. </p>
<p>For historical reasons, this function h is called a <strong>hypothesis</strong>.</p>
<p><img src="../../../../public/img/hypothesis-sl.png" alt="img"></p>
<p>When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a <strong>regression problem</strong>. </p>
<p>When $y$ can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a <strong>classification problem</strong>.</p>
<h5 id="the-cost-function">The Cost Function</h5>
<p>Back to the example of the house prices, with hypothesis:
$$
h_\theta(x) = \theta_0 + \theta_1x
$$
and with $\theta_i$ represent the <strong>parameters</strong> of the <strong>model</strong>, it is time to find out what $\theta_0$ and $\theta_1$ stand for.</p>
<p><img src="../../../../public/img/image-20211217153913519.png" alt="image-20211217153913519"></p>
<p>The mission is to <strong>find out $\theta_0$ and $\theta_1$</strong> to make our hypothesis function close to our tranning set.</p>
<p>Hence we have:
$$
J(\theta_0, \theta_1) = {1 \over 2m}\stackrel{m}{\sum_{i=1}}(h_\theta(x^{(i)}) \space\space - \space\space y^{(i)})^2
$$
We want the find out the <strong>minimization</strong> of the function:
$$
\substack{minimize\\theta_0,\theta_1} \space J(\theta_0, \theta_1)
$$
We call $J(\theta_0, \theta_1)$ a <strong>Cost function</strong> or <strong>Square error cost function</strong>.It is the most commonly used function for most linear regression function.</p>
<p>This takes an <strong>average difference</strong> $\frac{1}{2m}$ (actually a fancier version of an average, better than $\frac{1}{m}$) of all the results of the hypothesis with inputs from $x$&#39;s and the actual output $y$&#39;s.</p>
<p>The <strong>mean is halved($\frac{1}{2}$)</strong> as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term(因为平方函数的导数项将抵消掉$\frac{1}{2}$项).</p>
<h4 id="multivariate-linear-regression">Multivariate Linear Regression</h4>
<p>Linear regression with multiple variables is also known as &quot;multivariate linear regression&quot;.</p>
<p>We now introduce notation for equations where we can have any number of input variables.</p>
<p>$x_j^{(i)}=$ the value of feature $j$ in the $i$ th training example</p>
<p>$x^{(i)}=$ the input (features) of the $i$ th training example</p>
<p>$m=$ the number of training <strong>examples</strong></p>
<p>$n=$ the number of <strong>features</strong></p>
<p>The multivariable form of the <strong>hypothesis aka $h$</strong> function accommodating these multiple features is as follows:
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
$$
In order to develop intuition about this function, we can think about $\theta_0$ as the <strong>basic price</strong> of a house, $\theta_1$ as <strong>the price per square meter</strong>,  $\theta_2$ as the <strong>price per floor</strong>, etc.  </p>
<p>$x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc.</p>
<p>Using the definition of <strong>matrix multiplication</strong>, our <strong>multivariable hypothesis function</strong> can be concisely represented as:
$$
h_\theta(x) = 
[\theta_1 &amp; \theta_2 &amp; \theta_3 &amp;\cdots&amp;\theta_4]
\begin{bmatrix}
   x_0 \
   x_1 \
   x_2 \
   \vdots\
   x_n
\end{bmatrix}
=\theta^Tx
$$
This is a <strong>vectorization</strong> of our hypothesis function for one training example; see the lessons on vectorization to learn more.</p>
<p>Note that for convenience reasons in this course we assume $x_0^{(i)} =  for (i\in1,\dots,m)$.</p>
<p>This allows us to do matrix operations with theta and $x$. </p>
<p>Hence making the two vectors $\theta$ and $x^{(i)} $ match each other element-wise (that is, have the same number of elements: $n+1$). </p>
